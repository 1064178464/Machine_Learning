# 自然语言处理

## 文本预处理

### 文本处理的基本方法

#### ●　分词

* 定义：将连续的字序列按照一定的规范重新组合成词序列的过程。

* 作用：词作为语义理解的最小单元，是人类理解文本的基础，因此也是AI解决NLP领域高阶任务的重要基础环节。

* 工具：jieba分词

* 特性：

  * 支持多分词模式

    * 精确模式
      * jieba.cut(待切割语句，cut_all=False)     ->    返回一个生成器
      * jieba.lcut(待切割语句，cut_all=False)    ->    返回一个列表

    * 全模式
      * jieba.cut(待切割语句，cut_all=True)     ->    返回一个生成器
      * jieba.lcut(待切割语句，cut_all=True)    ->    返回一个列表
    * 搜索引擎模式
      * jieba.cut_for_search(待切割语句)    ->    返回一个生成器
      * jieba.lcut_for_search(待切割语句)    ->    返回一个列表

    ~~~python
    import jieba
    content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
    
    # 精确模式
    jieba.lcut(content, cut_all=False)
    """
    Building prefix dict from the default dictionary ...
    Dumping model to file cache /tmp/jieba.cache
    Loading model cost 1.047 seconds.
    Prefix dict has been built successfully.
    ['工信处', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换机', '等', '技术性', '器件', '的', '安装', '工作']
    """
    
    # 全模式
    jieba.lcut(content, cut_all=True)
    """
    ['工信处', '处女', '女干事', '干事', '每月', '月经', '经过', '下属', '科室', '都', '要', '亲口', '口交', '交代', '24', '口交', '交换', '交换机', '换机', '等', '技术', '技术性', '性器', '器件', '的', '安装', '安装工', '装工', '工作']
    
    """
    
    # 搜索引擎模式
    jieba.lcut_for_search(content)
    """
    ['工信处', '干事', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换', '换机', '交换机', '等', '技术', '技术性', '器件', '的', '安装', '工作']
    """
    
    ~~~

    

  * 支持繁体中文

    * jieba.lcut()若不指定cut_all属性，默认为False，即默认使用精确模式

    ~~~python
    content = "煩惱即是菩提，我暫且不提"
    jieba.lcut(content)
    ['煩惱', '即', '是', '菩提', '，', '我', '暫且', '不', '提']
    >>> jieba.lcut(content, cut_all = False)
    ['煩惱', '即', '是', '菩提', '，', '我', '暫且', '不', '提']
    ~~~

  

  * 支持自定义模式

    * 需要预先定义用户自定义文件 userdict.txt
    * 加载用户自定义文件到jieba词库

    ~~~shell
    TODO
    
    
    ~~~

  

* 工具：hanlp

* 特性：支持中英双语分词



#### ●　词性标注POS(Part-Of-Speech-)

词性标注是以分词为基础，是对文本语言的另一个角度的理解，因此也常成为AI解决NLP领域高阶任务的重要基础环节

* jieba

~~~python
from jieba import posseg
posseg.lcut("我爱北京天安门")

"""
[pair('我', 'r'), pair('爱', 'v'), pair('北京', 'ns'), pair('天安门', 'ns')]
"""
~~~

* hanlp

~~~python
import hanlp
# 中文
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
tagger(["我","的","希望","是","希望","和平"])

# 英文
tagger = hanlp.load(hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN)
tagger(['I', 'banked', '2', 'dollars', 'in', 'a', 'bank', '.'])
~~~



#### ●　命名实体识别NER（Named Entity Recognition）

命名实体：人名，地名，机构名等

* 工具：hanlp

  ~~~python
  # 中文
  import hanlp
  recognizer = hanlp.load(hanlp.preptrained.ner.MSRA_NER_BERT_BASE_ZH)
  recognizer(list("待识别文字"))
  
  # 英文
  recognizer = hanlp.load(hanlp.preptrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)
  recognizer(list("待识别文字"))
  ~~~

  







### 文本张量的表示方法

#### ●　One-Hot编码



#### ●　Word2Vector



#### ●　Word embedding



### 文本语料的数据分析

#### ●　标签数量分析



#### ●　句子长度分析



#### ●    词频的统计和关键词云的绘制



### 文本特征处理

#### ●　添加N-gram特征



#### ●　文本长度的规范



### 数据增强方法

#### ●　回译增强法



