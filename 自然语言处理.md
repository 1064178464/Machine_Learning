# 自然语言处理

## 文本预处理

### 文本处理的基本方法

#### ●　分词

* 定义：将连续的字序列按照一定的规范重新组合成词序列的过程。

* 作用：词作为语义理解的最小单元，是人类理解文本的基础，因此也是AI解决NLP领域高阶任务的重要基础环节。

* 工具：jieba分词

* 特性：

  * 支持多分词模式

    * 精确模式
      * jieba.cut(待切割语句，cut_all=False)     ->    返回一个生成器
      * jieba.lcut(待切割语句，cut_all=False)    ->    返回一个列表

    * 全模式
      * jieba.cut(待切割语句，cut_all=True)     ->    返回一个生成器
      * jieba.lcut(待切割语句，cut_all=True)    ->    返回一个列表
    * 搜索引擎模式
      * jieba.cut_for_search(待切割语句)    ->    返回一个生成器
      * jieba.lcut_for_search(待切割语句)    ->    返回一个列表

    ~~~python
    import jieba
    content = "工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"
    
    # 精确模式
    jieba.lcut(content, cut_all=False)
    """
    Building prefix dict from the default dictionary ...
    Dumping model to file cache /tmp/jieba.cache
    Loading model cost 1.047 seconds.
    Prefix dict has been built successfully.
    ['工信处', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换机', '等', '技术性', '器件', '的', '安装', '工作']
    """
    
    # 全模式
    jieba.lcut(content, cut_all=True)
    """
    ['工信处', '处女', '女干事', '干事', '每月', '月经', '经过', '下属', '科室', '都', '要', '亲口', '口交', '交代', '24', '口交', '交换', '交换机', '换机', '等', '技术', '技术性', '性器', '器件', '的', '安装', '安装工', '装工', '工作']
    
    """
    
    # 搜索引擎模式
    jieba.lcut_for_search(content)
    """
    ['工信处', '干事', '女干事', '每月', '经过', '下属', '科室', '都', '要', '亲口', '交代', '24', '口', '交换', '换机', '交换机', '等', '技术', '技术性', '器件', '的', '安装', '工作']
    """
    
    ~~~

    

  * 支持繁体中文

    * jieba.lcut()若不指定cut_all属性，默认为False，即默认使用精确模式

    ~~~python
    content = "煩惱即是菩提，我暫且不提"
    jieba.lcut(content)
    ['煩惱', '即', '是', '菩提', '，', '我', '暫且', '不', '提']
    >>> jieba.lcut(content, cut_all = False)
    ['煩惱', '即', '是', '菩提', '，', '我', '暫且', '不', '提']
    ~~~

  

  * 支持自定义模式

    * 需要预先定义用户自定义文件 userdict.txt
    * 加载用户自定义文件到jieba词库

    ~~~shell
    TODO
    
    
    ~~~

  

* 工具：hanlp

* 特性：支持中英双语分词



#### ●　词性标注POS(Part-Of-Speech Tagging)

词性标注是以分词为基础，是对文本语言的另一个角度的理解，因此也常成为AI解决NLP领域高阶任务的重要基础环节

* jieba

~~~python
from jieba import posseg
posseg.lcut("我爱北京天安门")

"""
[pair('我', 'r'), pair('爱', 'v'), pair('北京', 'ns'), pair('天安门', 'ns')]
"""
~~~

* hanlp

~~~python
import hanlp
# 中文
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
tagger(["我","的","希望","是","希望","和平"])

# 英文
tagger = hanlp.load(hanlp.pretrained.pos.PTB_POS_RNN_FASTTEXT_EN)
tagger(['I', 'banked', '2', 'dollars', 'in', 'a', 'bank', '.'])
~~~



#### ●　命名实体识别NER（Named Entity Recognition）

命名实体：人名，地名，机构名等

* 工具：hanlp

  ~~~python
  # 中文
  import hanlp
  recognizer = hanlp.load(hanlp.preptrained.ner.MSRA_NER_BERT_BASE_ZH)
  recognizer(list("待识别文字"))
  
  # 英文
  recognizer = hanlp.load(hanlp.preptrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN)
  recognizer(list("待识别文字"))
  ~~~

  

### 文本张量的表示方法

将词汇为表示成向量，称作词向量，再由各个词向量按顺序组成矩阵形成文本表示。

作用：能够使语言文本可以作为计算机处理程序的输入，进行接下来的一系列的解析工作。

#### ●　One-Hot编码

将每个词表示成具有n个元素的向量，这个词向量中只有一个元素是1，其他元素都是0，不同词汇元素为0的位置不同，其中n的大小是整个语料中不同词汇的总数.

~~~
["改变", "要", "如何", "起手"]`
==>

[[1, 0, 0, 0],
 [0, 1, 0, 0],
 [0, 0, 1, 0],
 [0, 0, 0, 1]]
~~~

~~~python
# 导入用于对象保存与加载的joblib
from sklearn.externals import joblib
# 导入keras中的词汇映射器Tokenizer
from keras.preprocessing.text import Tokenizer
# 假定vocab为语料集所有不同词汇集合
vocab = {"周杰伦", "陈奕迅", "王力宏", "李宗盛", "吴亦凡", "鹿晗"}
# 实例化一个词汇映射器对象
t = Tokenizer(num_words=None, char_level=False)
# 使用映射器拟合现有文本数据
t.fit_on_texts(vocab)

for token in vocab:
    zero_list = [0]*len(vocab)
    # 使用映射器转化现有文本数据, 每个词汇对应从1开始的自然数
    # 返回样式如: [[2]]是两个列表的嵌套, 取出其中的数字需要使用[0][0]
    # 由于编码从1开始, 故index要-1
    token_index = t.texts_to_sequences([token])[0][0] - 1
    zero_list[token_index] = 1
    print(token, "的one-hot编码为:", zero_list)

# 使用joblib工具保存映射器, 以便之后使用
tokenizer_path = "./Tokenizer"
joblib.dump(t, tokenizer_path)


"""
王力宏 的one-hot编码为: [1, 0, 0, 0, 0, 0]
吴亦凡 的one-hot编码为: [0, 1, 0, 0, 0, 0]
李宗盛 的one-hot编码为: [0, 0, 1, 0, 0, 0]
周杰伦 的one-hot编码为: [0, 0, 0, 1, 0, 0]
鹿晗 的one-hot编码为: [0, 0, 0, 0, 1, 0]
陈奕迅 的one-hot编码为: [0, 0, 0, 0, 0, 1]
"""
~~~

编码器的使用

~~~python
tokenizer_path = './Tokenizer'

# one-hot 编码器的使用
from sklearn.externals import joblib

t = joblib.load(tokenizer_path)
# 注意此处的token必须是之前的表中出现过的词，否则会报错
token = '李宗盛'
token_index = t.texts_to_sequences([token])[0][0]-1
zero_list = [0] * 6
zero_list[token_index] = 1
print(token, "的 one hot 编码为：", zero_list)

"""
李宗盛 的 one hot 编码为： [0, 0, 1, 0, 0, 0]
"""
~~~

报错演示！！！

~~~python
# one-hot 编码器的使用
from sklearn.externals import joblib

t = joblib.load(tokenizer_path)

# dog没有出现过
token = 'dog'

token_index = t.texts_to_sequences([token])[0][0]-1
zero_list = [0] * len(vocab)
zero_list[token_index] = 1
print(token, "的 one hot 编码为：", zero_list)

"""
报错！！！
Traceback (most recent call last):
  File "/usr/nw/text_preprocess.py", line 32, in <module>
    token_index = t.texts_to_sequences([token])[0][0]-1
IndexError: list index out of range
"""
~~~

优点：操作简单，便于理解。

缺点：完全割裂了词与词之间的联系，而且在大语料集下，每个向量的长度过大，占据大量内存。



#### ●　Word2Vector



##### CBOW（Continuous bag of words）模式：

两边预测中间



##### skipgram模式

中间预测两边



* 工具：fasttext



#### ●　Word embedding



### 文本数据分析

文本数据分析能有效帮助我们理解数据的语料，快速检查出语料中可能存在的问题，并指导后续模型训练过程中的一些超参的选择。

#### ●　标签数量分析

作用：查看样本分布，是否有必要做类别均衡

#### ●　句子长度分析

作用：统计语料中数据的句长，针对不同的数据集采用不同的模型

截断补齐时使用

#### ●    词频的统计和关键词云的绘制



### 文本特征处理

#### ●　添加N-gram特征

N-gram特征：相邻且共通出现的特征

一般N取2,3同时1也可以，再高就会由于算力限制原因无法实现。

#### ●　文本长度的规范



### 数据增强方法

#### ●　回译增强法

将文本使用Google Translate翻译为外文后再翻译回来，就可以得到语义相近内容不同的文本。

---



## RNN架构及其变体

RNN、LSTM、GRU模型

### RNN的定义

循环神经网络（Recurrent Neural Network）

一遍以序列数据为输入，通过网络内部的结构设计有效捕捉序列之间的关系特征，一般也是以序列形式进行输出。



### RNN的基本结构

简单的三层结构：输入层，隐层，输出层，循环在隐层。



### RNN模型的作用

RNN的循环机制使模型隐层上一时间步产生的结果, 能够作为当下时间步输入的一部分(当下时间步的输入除了正常的输入外还包括上一步的隐层输出)对当下时间步的输出产生影响。

每一步循环代表是一个时间步（在句子中可以理解为一个一个的单词）

因为RNN结构能够很好利用序列之间的关系, 因此针对自然界具有连续性的输入序列, 如人类的语言, 语音等进行很好的处理, 广泛应用于NLP领域的各项任务, 如文本分类, 情感分析, 意图识别, 机器翻译等。



### RNN模型的分类

- 按照输入和输出的结构进行分类:

  - N vs N - RNN：
    - 最基础的结构形式，由于输入输出等长，使得其使用范围较小，适用于生成等长的诗句。
  - N vs 1 - RNN：
    - 有N个输入对应只有1个输出，可用于用户意图识别、文本分类等，只需要一个输出，随后对输出进行判别。
  - 1 vs N - RNN
    - 唯一的一个输入对应N个输出，可用于将图片生成文字的任务。
  - N vs M - RNN
    - 输入输出端的长度都不做要求，也被称为Seq2Seq架构，由编码器和解码器两部分组成。可用于机器翻译、阅读理解、文本摘要等众多领域。

- 按照RNN的内部构造进行分类:

  - 传统RNN
  - LSTM
  - Bi-LSTM
  - GRU
  - Bi-GRU

  

### 传统RNN

内部结构

![avatar](http://121.199.45.168:8002/img/21.png)

![avatar](http://121.199.45.168:8002/img/22.png)



方块部分, 它的输入有两部分, 分别是h(t-1)以及x(t), 代表上一时间步的隐层输出, 以及此时间步的输入, 它们进入RNN结构体后, 会"融合"到一起, 这种融合我们根据结构解释可知, 是将二者进行拼接, 形成新的张量[x(t), h(t-1)], 之后这个新的张量将通过一个全连接层(线性层), 该层使用tanh作为激活函数, 最终得到该时间步的输出h(t), 它将作为下一个时间步的输入和x(t+1)一起进入结构体. 以此类推。



- 根据结构分析得出内部计算公式:

![avatar](http://121.199.45.168:8002/img/23.png)



解析：新输出 = tanh（权重*[新特征向量，上一次的输出 的拼接] + 偏置 ）



**代码演示：**

~~~python
# 导入工具包
import torch
import torch.nn as nn

# 5: 输入张量x的维度 input_size
# 6: 隐藏层中神经元的数量 hidden_size
# 1: 隐藏层的层数 num_layers
rnn = nn.RNN(5, 6, 1)

# 1: 输入序列的长度 Sequence Length 一句话有多长
# 3: batch size 有几句话
# 5: input size 
input = torch.randn(1, 3, 5)

# 1: 隐藏层数 * 网络方向数（单向为1，双向为2）num_layers * num_direction
# 3: batch size
# 6: 隐藏层中的神经元的数量（隐藏层的维度）
h0 = torch.randn(1, 3, 6)
output, hn = rnn(input, h0)

"""
>>> output
tensor([[[ 0.4282, -0.8475, -0.0685, -0.4601, -0.8357,  0.1252],
         [ 0.5758, -0.2823,  0.4822, -0.4485, -0.7362,  0.0084],
         [ 0.9224, -0.7479, -0.3682, -0.5662, -0.9637,  0.4938]]],
       grad_fn=<StackBackward>)

>>> hn
tensor([[[ 0.4282, -0.8475, -0.0685, -0.4601, -0.8357,  0.1252],
         [ 0.5758, -0.2823,  0.4822, -0.4485, -0.7362,  0.0084],
         [ 0.9224, -0.7479, -0.3682, -0.5662, -0.9637,  0.4938]]],
       grad_fn=<StackBackward>)
"""
~~~



**传统RNN的优缺点：**

优点：内部结构简单，对计算资源的要求低，在短序列(short sequence length)任务上效果优异

缺点：在处理较长的序列任务时，不能有效捕捉到序列之间的语义关联，同时反向传播过程中由于序列过长会造成梯度计算异常，要么梯			度消失，要么梯度爆炸。

- 什么是梯度消失或爆炸？
  - 根据反向传播算法和链式法则, 梯度的计算可以简化为以下公式:

![avatar](http://121.199.45.168:8002/img/RNN25.png)

- 其中sigmoid的导数值域是固定的, 在[0, 0.25]之间, 而一旦公式中的w也小于1, 那么通过这样的公式连乘后, 最终的梯度就会变得非常非常小, 这种现象称作梯度消失. 反之, 如果我们人为的增大w的值, 使其大于1, 那么连乘够就可能造成梯度过大, 称作梯度爆炸.

- 梯度消失或爆炸的危害:
  - 如果在训练过程中发生了梯度消失，权重无法被更新，最终导致训练失败; 
  - 梯度爆炸所带来的梯度过大，大幅度更新网络参数，在极端情况下，结果会溢出（NaN值）.

---



### LSTM

![avatar](http://121.199.45.168:8002/img/31.png)

LSTM（Long Short-Term Memory）也称长短时记忆结构, 它是传统RNN的变体, 与经典RNN相比能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象. 同时LSTM的结构更复杂, 它的核心结构可以分为四个部分去解析:

**遗忘门：**

![avatar](http://121.199.45.168:8002/img/32.png)

- 遗忘门结构分析:

  - 与传统RNN的内部结构计算非常相似, 首先将当前时间步输入x(t)与上一个时间步隐含状态h(t-1)拼接, 得到[x(t), h(t-1)], 然后通过一个全连接层做变换, 最后通过sigmoid函数进行激活得到f(t)。

  - 我们可以将f(t)看作是门值, 好比一扇门开合的大小程度, 门值都将作用在通过该扇门的张量, 遗忘门门值将作用的上一层的细胞状态上, 代表遗忘过去的多少信息, 又因为遗忘门门值是由x(t), h(t-1)计算得来的。

  - 整个公式意味着根据当前时间步输入和上一个时间步隐含状态h(t-1)来决定遗忘多少上一层的细胞状态所携带的过往信息。

    

**输入门:**

![avatar](http://121.199.45.168:8002/img/34.png)

- 输入门结构分析:
  - 输入门的计算公式有两个, 第一个就是产生输入门门值的公式, 它和遗忘门公式几乎相同, 区别只是在于它们之后要作用的目标上. 
  - 这个公式意味着输入信息有多少需要进行过滤. 输入门的第二个公式是与传统RNN的内部结构计算相同. 对于LSTM来讲, 它得到的是当前的细胞状态, 而不是像经典RNN一样得到的是隐含状态.

**细胞状态:**

 ![avatar](http://121.199.45.168:8002/img/35.png)

- 细胞状态更新分析:
  - 细胞更新的结构与计算公式非常容易理解, 这里没有全连接层, 只是将刚刚得到的遗忘门门值与上一个时间步得到的C(t-1)相乘, 再加上输入门门值与当前时间步得到的未更新C(t)相乘的结果. 最终得到更新后的C(t)作为下一个时间步输入的一部分. 整个细胞状态更新过程就是对遗忘门和输入门的应用.

**输出门:**

![avatar](http://121.199.45.168:8002/img/37.png)

- 输出门结构分析:
  - 输出门部分的公式也是两个, 第一个即是计算输出门的门值, 它和遗忘门，输入门计算方式相同. 
  - 第二个即是使用这个门值产生隐含状态h(t), 他将作用在更新后的细胞状态C(t)上, 并做tanh激活, 最终得到h(t)作为下一时间步输入的一部分.
  - 整个输出门的过程, 就是为了产生隐含状态h(t).



**代码演示**

~~~python
# 定义LSTM的参数含义: (input_size, hidden_size, num_layers)
# 定义输入张量的参数含义: (sequence_length, batch_size, input_size)
# 定义隐藏层初始张量和细胞初始状态张量的参数含义:
# (num_layers * num_directions, batch_size, hidden_size)

import torch.nn as nn
import torch
lstm = nn.LSTM(5, 6, 2)
input = torch.randn(1, 3, 5)
h0 = torch.randn(2, 3, 6)
c0 = torch.randn(2, 3, 6)
output, (hn, cn) = lstm(input, (h0, c0))

"""
>>>output
tensor([[[ 0.0447, -0.0335,  0.1454,  0.0438,  0.0865,  0.0416],
         [ 0.0105,  0.1923,  0.5507, -0.1742,  0.1569, -0.0548],
         [-0.1186,  0.1835, -0.0022, -0.1388, -0.0877, -0.4007]]],
       grad_fn=<StackBackward>)
       
>>> hn
tensor([[[ 0.4647, -0.2364,  0.0645, -0.3996, -0.0500, -0.0152],
         [ 0.3852,  0.0704,  0.2103, -0.2524,  0.0243,  0.0477],
         [ 0.2571,  0.0608,  0.2322,  0.1815, -0.0513, -0.0291]],

        [[ 0.0447, -0.0335,  0.1454,  0.0438,  0.0865,  0.0416],
         [ 0.0105,  0.1923,  0.5507, -0.1742,  0.1569, -0.0548],
         [-0.1186,  0.1835, -0.0022, -0.1388, -0.0877, -0.4007]]],
       grad_fn=<StackBackward>)
       
>>> cn
tensor([[[ 0.8083, -0.5500,  0.1009, -0.5806, -0.0668, -0.1161],
         [ 0.7438,  0.0957,  0.5509, -0.7725,  0.0824,  0.0626],
         [ 0.3131,  0.0920,  0.8359,  0.9187, -0.4826, -0.0717]],

        [[ 0.1240, -0.0526,  0.3035,  0.1099,  0.5915,  0.0828],
         [ 0.0203,  0.8367,  0.9832, -0.4454,  0.3917, -0.1983],
         [-0.2976,  0.7764, -0.0074, -0.1965, -0.1343, -0.6683]]],
       grad_fn=<StackBackward>)
"""
~~~



**LSTM的优势和缺点：**

优势：环节梯度消失、爆炸问题，能有效捕捉长序列的语义关联

缺点：计算复杂

---



### Bi-LSTM

- **什么是Bi-LSTM ?**
  - Bi-LSTM即双向LSTM, 它没有改变LSTM本身任何的内部结构。
  - 只是将LSTM应用两次且方向不同, 再将两次得到的LSTM结果进行拼接作为最终输出。

![avatar](http://121.199.45.168:8002/img/38.png)

- **Bi-LSTM结构分析:**
  - 我们看到图中对"我爱中国"这句话或者叫这个输入序列, 进行了从左到右和从右到左两次LSTM处理, 将得到的结果张量进行了拼接作为最终输出。
  - 这种结构能够捕捉语言语法中一些特定的前置或后置特征, 增强语义关联,但是模型参数和**计算复杂度也随之增加了一倍**, 一般**需要对语料和计算资源进行评估后决定是否使用该结构。**

---

### GRU

![avatar](http://121.199.45.168:8002/img/gru.png)

输入的数据有两个 ht-1，xt

- GRU（Gated Recurrent Unit）也称门控循环单元结构, 它也是传统RNN的变体, 同LSTM一样能够有效捕捉长序列之间的语义关联, 缓解梯度消失或爆炸现象. 同时它的结构和计算要比LSTM更简单, 它的核心结构可以分为两个部分去解析:

  - 更新门

  - 重置门

![avatar](http://121.199.45.168:8002/img/gru2.png)

- 内部结构分析:

  - 和之前分析过的LSTM中的门控一样, 首先计算更新门和重置门的门值, 分别是z(t)和r(t), 计算方法就是使用X(t)与h(t-1)拼接进行线性变换, 再经过sigmoid激活. 
  - 之后重置门门值作用在了h(t-1)上, 代表控制上一时间步传来的信息有多少可以被利用. 
  - 接着使用这个重置后的h(t-1)进行基本的RNN计算, 即与x(t)拼接进行线性变化, 经过tanh激活, 得到新的h(t).
  - 最后更新门的门值会作用在新的h(t)，而1-门值会作用在h(t-1)上, 随后将两者的结果相加, 得到最终的隐含状态输出h(t), 这个过程意味着更新门有能力保留之前的结果, 当门值趋于1时, 输出就是新的h(t), 而当门值趋于0时, 输出就是上一时间步的h(t-1).

  **GRU的优势和缺点：**

  优点：在捕捉长序列语义关联时，能有效抑制梯度消失或爆炸，参数更少。

  缺点：不能完全解决梯度消失问题，同时其作用RNN的变体, 有着RNN结构本身的一大弊端, 即不可并行计算。

  

---

### Bi-GRU

- Bi-GRU与Bi-LSTM的逻辑相同, 都是不改变其内部结构, 而是将模型应用两次且方向不同, 再将两次得到的LSTM结果进行拼接作为最终输出. 具体参见Bi-LSTM.

---

  

## 注意力机制

把精力放在事物最有辨识度的地方

### 什么是注意力:

- 我们观察事物时，之所以能够快速判断一种事物(当然允许判断是错误的), 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果. 正是基于这样的理论，就产生了注意力机制。



- 什么是注意力计算规则:

  - 它需要三个指定的输入 
    - 查询向量Q(query)
    - 键向量K(key)
    - 值向量V(value)
    - 如果Q != K 且 K=V 称之为常规注意力机制
    - 如果Q = K=V 称之为自注意力机制
  - 通过计算公式得到注意力的结果, 这个结果代表query在key和value作用下的注意力表示. 当输入的Q=K=V时, 称作自注意力计算规则。

  

- 常见的注意力计算规则:

- 常用

> > - 将Q，K进行纵轴拼接, 做一次线性变化, 再使用softmax处理获得结果最后与V做张量乘法.

![avatar](http://121.199.45.168:8002/img/51.png)



- 不常用

> > - 将Q，K进行纵轴拼接, 做一次线性变化后再使用tanh函数激活, 然后再进行内部求和, 最后使用softmax处理获得结果再与V做张量乘法.

![avatar](http://121.199.45.168:8002/img/52.png)



- 常用（Transformer机制中使用）

> > - 将Q与K的转置做点积运算, 然后除以一个缩放系数, 再使用softmax处理获得结果最后与V做张量乘法.

![avatar](http://121.199.45.168:8002/img/53.png)



说明：当注意力权重矩阵和V都是三维张量且第一维代表为batch条数时, 则做bmm运算.bmm是一种特殊的张量乘法运算.

bmm运算规则：第一维Batch不参与运算

~~~shell
# 如果参数1形状是(b × n × m), 参数2形状是(b × m × p), 则输出为(b × n × p)
>>> input = torch.randn(10, 3, 4)
>>> mat2 = torch.randn(10, 4, 5)
>>> res = torch.bmm(input, mat2)
>>> res.size()
torch.Size([10, 3, 5])
~~~

### 什么是注意力机制

- 注意力机制是注意力计算规则能够应用的深度学习网络的载体, 同时包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体. 使用自注意力计算规则的注意力机制称为自注意力机制。

- 说明: NLP领域中, 当前的注意力机制大多数应用于seq2seq架构, 即编码器和解码器模型。



### 注意力机制的作用

- 在解码器端的注意力机制: 能够根据模型目标有效的聚焦编码器的输出结果, 当其作为解码器的输入时提升效果. 改善以往编码器输出是单一定长张量, 无法存储过多信息的情况。
- 在编码器端的注意力机制: 主要解决表征问题, 相当于特征提取过程, 得到输入的注意力表示. 一般使用自注意力(self-attention)。



### 注意力机制实现步骤

- 第一步: 根据注意力计算规则, 对Q，K，V进行相应的计算。
- 第二步: 根据第一步采用的计算方法, 如果是拼接方法，则需要将Q与第二步的计算结果再进行拼接, 如果是转置点积, 一般是自注意力, Q与V相同, 则不需要进行与Q的拼接。
- 第三步: 最后为了使整个attention机制按照指定尺寸输出, 使用线性层作用在第二步的结果上做一个线性变换, 得到最终对Q的注意力表示。



~~~python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Attn(nn.Module):
    def __init__(self, query_size, key_size, value_size1, value_size2, output_size):
        """初始化函数中的参数有5个, query_size代表query的最后一维大小
           key_size代表key的最后一维大小, value_size1代表value的倒数第二维大小, 
           value = (1, value_size1, value_size2)
           value_size2代表value的倒数第一维大小, output_size输出的最后一维大小"""
        super(Attn, self).__init__()
        # 将以下参数传入类中
        self.query_size = query_size
        self.key_size = key_size
        self.value_size1 = value_size1
        self.value_size2 = value_size2
        self.output_size = output_size

        # 初始化注意力机制实现第一步中需要的线性层.
        self.attn = nn.Linear(self.query_size + self.key_size, value_size1)

        # 初始化注意力机制实现第三步中需要的线性层.
        self.attn_combine = nn.Linear(self.query_size + value_size2, output_size)


    def forward(self, Q, K, V):
        """forward函数的输入参数有三个, 分别是Q, K, V, 根据模型训练常识, 输入给Attention机制的
           张量一般情况都是三维张量, 因此这里也假设Q, K, V都是三维张量"""

        # 第一步, 按照计算规则进行计算, 
        # 我们采用常见的第一种计算规则
        # 将Q，K进行纵轴拼接, 做一次线性变化, 最后使用softmax处理获得结果
        attn_weights = F.softmax(
            self.attn(torch.cat((Q[0], K[0]), 1)), dim=1)

        # 然后进行第一步的后半部分, 将得到的权重矩阵与V做矩阵乘法计算, 
        # 当二者都是三维张量且第一维代表为batch条数时, 则做bmm运算
        attn_applied = torch.bmm(attn_weights.unsqueeze(0), V)

        # 之后进行第二步, 通过取[0]是用来降维, 根据第一步采用的计算方法, 
        # 需要将Q与第一步的计算结果再进行拼接
        output = torch.cat((Q[0], attn_applied[0]), 1)

        # 最后是第三步, 使用线性层作用在第三步的结果上做一个线性变换并扩展维度，得到输出
        # 因为要保证输出也是三维张量, 因此使用unsqueeze(0)扩展维度
        output = self.attn_combine(output).unsqueeze(0)
        return output, attn_weights
~~~































