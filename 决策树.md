# 信息熵
度量一个数据集包含的信息多少。

# 如何计算
计算每一种的信息占总信息数量的比重。
当前样本集合D
第k类样本
占的比例<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F.png?raw=true"></img>

公式为：<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E4%BF%A1%E6%81%AF%E7%86%B5%E5%85%AC%E5%BC%8F.png?raw=true"></img>  
D是当前样本集所有样本
<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E7%AC%ACk%E7%B1%BB%E6%A0%B7%E6%9C%AC%E6%95%B0%E9%87%8F.png?raw=true"></src>是第k类  

则样本集D的信息熵为：<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E6%A0%B7%E6%9C%AC%E9%9B%86D%E7%9A%84%E4%BF%A1%E6%81%AF%E7%86%B5.png?raw=true"></src>

Ent(D)的值越小，D的纯度越高

# 注意
信息熵越大，说明包含的信息越多，纯度就越低。
信息熵的大小，和包含的信息种类有关，和信息的总数量没有太大关系。
信息熵越大，也说明信息的混乱程度越高；反之，信息有序性越高。

# 信息增益
当某个特征作为条件的时候，使得类信息熵减少的程度。
信息增益越大，用条件所获得的 纯度提升  越大。

# 信息增益如何计算
信息增益 = 目标值的信息熵 - 条件熵（在某个特征的条件下，目标信息熵的大小）

离散属性a
V个可能的取值
表示为：<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E7%A6%BB%E6%95%A3%E5%B1%9E%E6%80%A7a%E6%9C%89V%E4%B8%AA%E5%8F%AF%E8%83%BD%E7%9A%84%E5%8F%96%E5%80%BC.png?raw=true"></img>

使用属性a对样本集D进行划分，会产生V个可能的值（分支节点）。第V个分支节点包含了 样本集D 在 属性a 上取值为 <img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/a%E5%B1%9E%E6%80%A7v%E7%9A%84%E5%8F%96%E5%80%BC.png?raw=true"></img>
TODO:划分权重
 

