# 常用损失函数

## 分类任务

### 多分类问题

对于多分类问题，在模型中一般会使用softmax函数将logits映射成每个类对应的概率，但对单个样本进行分类时，一般此样本只会属于某一个类别，因此需要对 label 做 one-hot 编码。

**交叉熵损失函数（softmax损失）**

$$
\mathcal{L}=-\sum_{i=1}^{n} \mathbf{y}_{i} \ln \left(S\left(f_{\theta}\left(\mathbf{x}_{i}\right)\right)\right)
$$

其中：

yi：是一个向量，内容为one-hot编码后的样本对应的真实label

fθ(xi)：模型对样本xi属于各个类别的预测分数

S()：softmax函数，将分数映射成概率

**注意！在TensorFlow中，交叉熵损失中 log() 是以 e 为底，故在此将公式中的log写为 ln()**



### 二分类问题

对于二分类问题而言，不再使用softmax函数做概率映射，而是使用sigmoid函数做概率映射

**二项交叉熵损失函数**
$$
\mathcal{L}=-y \ln \hat{y}-(1-y) \ln (1-\hat{y})
$$
对于二分类问题而言y是样本x属于某类的概率，因为y是对事实的描述，所有y只会取0表示样本不属于这一类或取1表示属于这一类。

y^则为模型给出的概率，因为是用sigmoid做映射，y^可能是0-1之间内的任意值。

例：

y = [ [0], [1] ]

y^ = [ [0.4], [0.6] ]

由此可见对于L来说，当y取0时与y取1时的结果是一致的都是( -ln(0.6) )
$$
\mathcal{L}=-0 \ln0.4 \ -(1-0) \ln (1-0.4)=-\ln(0.6)
\\
\mathcal{L}=-1 \ln0.6 \ -(1-1) \ln (1-0.4)=-\ln(0.6)
$$


## 回归任务

### MAE（L1 LOSS）

使用真实值与输出值之**差**的**绝对值之和**来衡量误差
$$
\mathcal{L}=\frac{1}{n} \sum_{i=1}^{n}\left|y_{i}-f_{\theta}\left(x_{i}\right)\right|
$$
<img src="https://github.com/FURYTAIL/Machine_Learning/blob/main/img/L1Loss.png?raw=true"/>

特点是具有稀疏性，常作为惩罚项添加到其他的损失函数中

其最大的问题在于该函数在0点是连续但不可导的，这会导致最小值被直接跳过

解决0点不可导的方法：

1. 坐标轴下降法
2. 次梯度下降法
3. Proximal Algorithm



### MSE（L2 Loss）

使用真实值与输出值之**差**的**平方和**来衡量误差

$$
\mathcal{L}=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}
$$

<img src="https://github.com/FURYTAIL/Machine_Learning/blob/main/img/L2Loss.png?raw=true"/>

特点：也常作为惩罚项

最大的问题在于当预测值和目标值相差过大时，容易发生梯度爆炸，也就是说L2 Loss对异常值敏感

当目标值是一个离群值时，可能会有MSE很高但模型实际效果很好的情况出现

教训：在评价模型的性能时，单纯从一个指标来看是不足以反映出模型的问题的，要尽可能使用互补的评价策略



### Smooth L1

$$
\operatorname{smooth}_{L_{1}}(x)=\left\{\begin{array}{ll}
0.5 x^{2} & \text { if }|x|<1 \\
|x|-0.5 & \text { otherwise }
\end{array}\right.
$$

<img src="https://github.com/FURYTAIL/Machine_Learning/blob/main/img/SmoothL1.png?raw=true"/>

实际上是一个由类L1 Loss和类L2 Loss组成的分段函数，当|x|<1时使用类L2 Loss，可以克服L1 Loss在0点不可导的问题，|x|>=1时使用类L1 Loss可以克服L2 Loss遇离群值引发梯度爆炸的问题



