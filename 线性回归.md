# 线性回归

## 什么是线性回归
线性回归 是利用函数对自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式。  
注意：回归问题数据是连续的。
只有一个自变量的场景叫单变量回归。
多于一个自变量的场景叫多变量回归



## 线性回归有什么作用
线性回归可以用来做回归（即预测）。

## 线性回归方程
<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%96%B9%E7%A8%8B2.png?raw=true">线性回归方程</img>  
也可以写成  
<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%96%B9%E7%A8%8B1.png?raw=true">线性回归方程</img>
w是特征权重  
x是特征值  
b是偏置  

## 线性回归模型分类
线性模型：自变量（特征值）x只被一个w所影响。  
非线性模型：自变量（特征值）x被多个因素影响。  

## 线性回归的流程和实现
流程：
step1:先调用sklearn.linear_model 中的LinearRegression
step2:创建数据集
step3:实例化API
step4:使用fit方法进行训练
step5:用estimator.coef_代表系数
step6:用estimator..intercept_代表偏置
step7:estimator.predict([]))来预测，参数都放在一个列表当中（这样就不受到参数个数的限制）

实现：
```python
# 线性回归

# coding:utf-8
from sklearn.linear_model import LinearRegression

x = [
    [80, 86],
    [82, 80],
    [85, 78],
    [90, 90],
    [86, 82],
    [82, 90],
    [78, 80],
    [92, 94]
]
y = [84.2, 80.6, 80.1, 90, 83.2, 87.6, 79.4, 93.4]

# 实例化API
estimator = LinearRegression()

# 使用fit方法进行训练
estimator.fit(x, y)

# 打印对应的系数
print("线性回归的系数是：\n", estimator.coef_)

# 打印偏置
print("偏置", estimator.intercept_)

# 打印预测结果
print("预测的结果：", estimator.predict([[100, 80]]))

```
打印结果：
线性回归的系数是：
 [0.3 0.7]
偏置 -1.4210854715202004e-14
预测的结果： [86.]

Process finished with exit code 0

# 损失函数
因为线性回归的公式  
<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%96%B9%E7%A8%8B1.png?raw=true">线性回归方程</img>  
中已知x,未知w1,w2,w3等权重，和b，w和b决定了y的值（w和b的值使y距离真实值偏差有大有小），这时就引入了损失函数的概念。  
损失函数的公式为：<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png?raw=true"></img>  
其中：  
<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/hx.png?raw=true"></img>代表预测值
<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/yi.png?raw=true"></img>代表真实值
这个方法为最小二乘法（即最小化误差平方和）

# 损失函数的作用
求解模型参数 w和b。  
但是我们创立模型的目的肯定是让模型和真实值之间缠距最小。因此为了让损失函数的值（即预测值和真实值之间的差距）最小，就要对损失函数求导。  

# 求解损失函数的两种方法
##正规方程
将损失函数看成y=a^2,此时导数有极小值点，在损失函数中也为最小值点。导数为0，即求得最小值点。  
<img src="https://github.com/BeGentleman/Machine_Learning/blob/main/img/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%8E%A8%E5%AF%BC.png?raw=true"></img>

##梯度下降
